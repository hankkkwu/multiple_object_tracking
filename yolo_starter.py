# -*- coding: utf-8 -*-
"""yolo_Starter.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1sOAnahzS6OpG8Btcv1ZfTMBJKkQVuNBR

# Welcome to the YOLO Workshop!

In this workshop, we will learn how to use the YOLO algorithm for object detection

## System Setup

Since we will work with external files, you will need to **link this Google Colab Notebook file to your Google Drive Account**.

This is done with the following piece of code.
If you're using Jupyter Notebook on your computer, you don't need to do this
"""

import os
from google.colab import drive
drive.mount('/content/drive', force_remount=False)

os.chdir("/content/drive/My Drive/tracking_course/Detection/yolo_workshop")

"""**ABOUT GPU** <p>

**YOLO** is an object detection algorithm.
Like most of them, it works better on GPU.
GPU will allow parallel computing (instead of sequential). You will not have a vector of operation, but a matrix of operations.<p>
![CPUvsGPUÃ¢â‚¬Â¦](https://www.nvidia.fr/docs/IO/144175/cpu-and-gpu.jpg)

Instead of 1 frame per second, you can run at 50 or 60 frames per second.

The version we'll use is developed by OpenCV.
OpenCV has a DNN (Deep Neural Networks) module that includes popular obstacle detection algorithms we saw in course.
We'll use something similar as [this post](https://www.pyimagesearch.com/2017/08/21/deep-learning-with-opencv/)


*   If you want to work on CPU, skip this section

*   If you want to use GPU, you will need OpenCV > 4.2.0 and compatible CUDA/CUDNN

As of today (March 2020), the preinstalled version of OpenCV and CUDA in Google Colab are unsufficient. We work on very recent libraries.<p>

**I will not cover installation of CUDA/CUDNN in this post. We'll use super-fast CPU.
If you want to use GPU, I recommend you exit Colab and try to run on your own machine or AWS.**

## Import the necessary libraries
We will need OpenCV, Matplotlib, and NumPy
"""

# your code here
import cv2
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.image as mpimg

"""## Define the class YOLO and the init() function"""

class YOLO():
    def __init__(self):
        """
        - YOLO takes an image as input. We should set the dimension of the image to a fixed number.
        - The default choice is often 416x416.
        - YOLO applies thresholding and non maxima suppression, define a value for both
        - Load the classes (.names), model configuration (cfg file) and pretrained weights (weights file) into variables
        - If the image is 416x416, the weights must be corresponding to that image
        - Load the network with OpenCV.dnn function
        """
        # TODO
        self.confThreshold = 0.6
        self.nmsThreshold = 0.5
        self.inpWidth = 320
        self.inpHeight = 320
        classesFile = "/content/drive/My Drive/tracking_course/Detection/yolo_workshop/coco.names"
        self.classes = None
        with open(classesFile,'rt') as f:
            self.classes = f.read().rstrip('\n').split('\n')

        modelConfiguration = "/content/drive/My Drive/tracking_course/Detection/yolo_workshop/yolov3.cfg"
        modelWeights = "/content/drive/My Drive/tracking_course/Detection/yolo_workshop/yolov3.weights"
        self.net = cv2.dnn.readNetFromDarknet(modelConfiguration, modelWeights)
        self.net.setPreferableBackend(cv2.dnn.DNN_BACKEND_OPENCV)
        self.net.setPreferableTarget(cv2.dnn.DNN_TARGET_CPU)

    def getOutputsNames(self):
        '''
        Get the names of the output layers
        '''
        # Get the names of all the layers in the network
        layersNames = self.net.getLayerNames()
        # Get the names of the output layers, i.e. the layers with unconnected outputs
        return [layersNames[i[0] - 1] for i in self.net.getUnconnectedOutLayers()]


    def drawPred(self, frame, classId, conf, left, top, right, bottom):
        '''
        Draw a bounding box around a detected object given the box coordinates
        Later, we could repurpose that to display an ID
        '''
        
        # for drawing bounding box with different color on different class of obstacles
        # np.random.seed(27)
        # COLORS = np.random.randint(0, 255, size=(len(self.classes), 3), dtype="uint8")
        # color = [int(c) for c in COLORS[classId]]
        
        # Draw a bounding box.
        # your code here
        color = (255,0,0)
        cv2.rectangle(frame, (left, top), (right, bottom), color, 3)
        
        # Get the label for the class name and its confidence
        # your code here
        text = "{}: {:.4f}".format(self.classes[classId], conf)
        
        #Display the label at the top of the bounding box
        # your code here
        # cv2.putText(frame, text, (left, top), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255,255,255), 2)
    
        return frame
    

    def postprocess(self, frame, outs):
        """
        Postprocessing step. Take the output out of the neural network and interpret it.
        We should use that output to apply NMS thresholding and confidence thresholding
        We should use the output to draw the bounding boxes using the dramPred function
        """
        frameHeight = frame.shape[0]
        frameWidth = frame.shape[1]
        classIds = []
        confidences = []
        boxes = []
        # Scan through all the bounding boxes output from the network and keep only the
        # ones with high confidence scores. Assign the box's class label as the class with the highest score.
        # your code here
        # loop over each of the layer output (I guess the outs is the number of anchor boxes)
        for output in outs:
          # loop over each of the detection
          for detection in output:
            # extract the class ID and confidence of the current object detection
            # the detection is an array of [bx, by, bw, bh, Pc, c1, c2, ..., c80]
            # Pc is the probability that there is an object
            scores = detection[5:]
            classID = np.argmax(scores)
            confidence = scores[classID]
            
            if confidence > self.confThreshold:
              center_x = int(detection[0] * frameWidth)
              center_y = int(detection[1] * frameHeight)
              width = int(detection[2] * frameWidth)
              height = int(detection[3] * frameHeight)
              left = int(center_x - width / 2)
              top = int(center_y - height / 2)
    
              classIds.append(classID)
              confidences.append(float(confidence))
              boxes.append([left, top, width, height])
    
        # Perform non maximum suppression to eliminate redundant overlapping boxes with
        # lower confidences.
        # your code here
        idxs = cv2.dnn.NMSBoxes(boxes, confidences, self.confThreshold, self.nmsThreshold)
    
        # get the bounding bxoes after performing non maximum suppression
        # your code here
        output_boxes = []
        if len(idxs) > 0:
          for i in idxs.flatten():  # idxs = [[1],[2],[5],...], idxs.flatten() = [1,2,5,...]
            output_boxes.append(boxes[i])
            left = boxes[i][0]
            top = boxes[i][1]
            width = boxes[i][2]
            height = boxes[i][3]
            right = left + width
            bottom = top + height
            frame = self.drawPred(frame, classIds[i], confidences[i], left, top, right, bottom)
        
        output_image = frame
        return output_image, output_boxes

    def inference(self,image):
        """
        Main loop.
        Input: Image
        Output: Frame with the drawn bounding boxes
        """
        # Create a 4D blob from a frame.
        blob = cv2.dnn.blobFromImage(image, scalefactor=1/255.0, size=(320,320))
        # Sets the input to the network
        self.net.setInput(blob)
        # Runs the forward pass to get output of the output layers
        layerOutputs = self.net.forward(self.getOutputsNames())
        # Remove the bounding boxes with low confidence using non-maximum suppression
        final_frame, boxes = self.postprocess(image, layerOutputs)
    
        return final_frame, boxes

